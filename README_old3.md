# ML4T - Machine Learning for Trading

Ein fortschrittliches System zur Analyse von Handelsdaten und Generierung von Handelssignalen mit maschinellem Lernen.

## Features

- Echtzeit-Marktdatenanalyse fÃ¼r AAPL, MSFT, GOOGL, NVDA und TSLA
- Technische Indikatoren (SMA, RSI) mit pandas-ta
- Machine Learning Vorhersagen mit TensorFlow/Keras
- Interaktives Trading-Dashboard mit Dash
- Detaillierte textuelle Analysen fÃ¼r jedes Symbol
- Automatisierte Handelssignalgenerierung mit visueller Darstellung
- Ein-/Ausblendbare Handelssignale im Chart

## Installation

1. Python 3.10+ wird empfohlen
2. Virtuelle Umgebung erstellen:
```bash
uv venv .venv
```

3. AbhÃ¤ngigkeiten installieren:
```bash
.venv\Scripts\activate
uv pip install -r requirements.txt
```

## Projektstruktur

Die detaillierte Projektstruktur finden Sie in der [project_structure.md](./project_structure.md) Datei.

## Verwendung

1. Hauptanalyse starten:
```bash
python master_analysis.py
```

2. Dashboard aufrufen:
- Trading Dashboard: http://localhost:8050/

## Tests

FÃ¼hren Sie die Tests aus mit:
```bash
pytest tests/ -v
```



## Installation

1. Repository klonen:
```bash
git clone https://github.com/yourusername/NextTick6.git
cd NextTick6
```

2. AbhÃ¤ngigkeiten installieren:
```bash
uv pip install -r requirements.txt
```

3. Entwicklungsserver starten:
```bash
npm run dev
```

## Entwicklung

- `npm run dev`: Startet den Entwicklungsserver
- `npm run build`: Erstellt eine Produktionsversion
- `npm run test`: FÃ¼hrt Tests aus
- `npm run lint`: FÃ¼hrt Linting durch

## Projektstruktur

Die detaillierte Projektstruktur finden Sie in der [project_structure.md](./project_structure.md) Datei.

## Tests

Alle Tests kÃ¶nnen mit dem folgenden Befehl ausgefÃ¼hrt werden:
```bash
npm test
```

## Lizenz

MIT

# NextTick6 Trading Analysis System

Ein fortschrittliches System zur Analyse von Handelsdaten und Generierung von Handelssignalen mit maschinellem Lernen.

## Features

- Echtzeit-Marktdatenanalyse fÃ¼r AAPL, MSFT, GOOGL, NVDA und TSLA
- Technische Indikatoren (SMA, RSI) mit pandas-ta
- Machine Learning Vorhersagen mit TensorFlow/Keras
- Interaktives Trading-Dashboard mit Dash
- Detaillierte textuelle Analysen fÃ¼r jedes Symbol
- Automatisierte Handelssignalgenerierung mit visueller Darstellung
- Ein-/Ausblendbare Handelssignale im Chart

## Installation

1. Python 3.10+ wird empfohlen
2. Virtuelle Umgebung erstellen:
```bash
uv venv .venv
```

3. AbhÃ¤ngigkeiten installieren:
```bash
.venv\Scripts\activate
uv pip install -r requirements.txt
```

## Projektstruktur

```
ml4t_project/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ market_analyzer.py    # Marktanalyse und technische Indikatoren
â”‚   â””â”€â”€ text_analyzer.py      # Textuelle Analysen
â”œâ”€â”€ data/
â”‚   â””â”€â”€ market_data.py       # Datenbeschaffung und -verarbeitung
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ml_models.py         # Machine Learning Modelle
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ dashboard.py         # Trading Dashboard
â””â”€â”€ exports/                 # Generierte Analysen und Charts
```

## Verwendung

1. Hauptanalyse starten:
```bash
python master_analysis.py
```

2. Dashboard aufrufen:
- Trading Dashboard: http://localhost:8050/

## Aktuelle Updates

- Vereinfachung des Dashboards mit Fokus auf Trading-Funktionen
- Verbesserte Darstellung der Handelssignale mit Ein-/Ausblendoption
- Ersetzung von TA-Lib durch pandas-ta fÃ¼r technische Indikatoren
- Implementierung detaillierter textueller Analysen
- Verbessertes Fehlerhandling und Logging
- Optimierte Numpy-Version (1.23.5) fÃ¼r KompatibilitÃ¤t

## Tests

```bash
pytest tests/
```

## Lizenz

MIT

## Autor

[Ihr Name]

# ML4T â€“ Machine Learning for Trading (LSTM Forecasting)

Dieses Projekt ist eine modulare ML-Pipeline zur Vorhersage von Marktbewegungen auf Basis historischer Finanzdaten. Es basiert auf dem ML4T-Workflow (Machine Learning for Trading) und verwendet Deep Learning (LSTM) zur Erstellung von Preisvorhersagen, die zu Handelssignalen (Buy/Sell) fÃ¼hren. Die Signale werden visuell dargestellt und Ã¼ber Backtesting auf historische Performance geprÃ¼ft.

## ğŸ”„ Projekt-Pipeline (Phasen)

1. ğŸ“¥ **Datenbeschaffung** â€“ Laden von Yahoo Finance-Daten (`yfinance`)
2. ğŸ§® **Feature Engineering** â€“ Berechnung technischer Indikatoren (`ta`)
3. ğŸ”§ **Preprocessing** â€“ Normalisierung, Sequenzierung
4. ğŸ¤– **Modelltraining** â€“ LSTM-Vorhersage-Modell (`keras`/`torch`)
5. ğŸ“Š **Evaluation** â€“ Testdaten-Auswertung
6. ğŸ“ˆ **Signal-Generierung** â€“ Buy/Sell-Logik aus Vorhersagen
7. ğŸ¨ **Visualisierung** â€“ Preis, Vorhersage & Signale
8. ğŸ§ª **Backtest** â€“ Simulierter Handel mit Ergebnisanalyse
9. ğŸ’¾ **Export** â€“ Modell & Ergebnisse speichern

## ğŸ“ Projektstruktur

Die Projektstruktur ist in zwei Hauptbereiche unterteilt:

### Test-Implementierung vs. Test-AusfÃ¼hrung

Das Projekt unterscheidet zwischen Test-Implementierungen und deren AusfÃ¼hrung:

1. **`ml4t_project/testing/`**: Test-Implementierungen
   - EnthÃ¤lt die eigentlichen Test-Implementierungen/Klassen
   - Stellt wiederverwendbare Test-Werkzeuge bereit
   - Implementiert die Test-Logik und FunktionalitÃ¤t
   - Hauptklassen:
     - `LoadStressTester`: Last- und Stress-Tests
     - `PerformanceTester`: Performance-Tests
     - `StabilityTester`: StabilitÃ¤tstests
     - `LoadTester`: Last-Tests
     - `MigrationStressTester`: Migrations-Stress-Tests
     - `ABTester`: A/B-Tests

2. **`ml4t_project/tests/`**: Test-AusfÃ¼hrung
   - EnthÃ¤lt die Unit-Tests und Integrationstests
   - Testet die FunktionalitÃ¤t der Test-Werkzeuge
   - Validiert das Verhalten der Implementierungen
   - Beispieldateien:
     - `test_load_stress.py`: Tests fÃ¼r LoadStressTester
     - `test_performance.py`: Tests fÃ¼r PerformanceTester
     - `test_stability.py`: Tests fÃ¼r StabilityTester
     - `test_migration_stress.py`: Tests fÃ¼r MigrationStressTester
     - `test_ab_testing.py`: Tests fÃ¼r ABTester

Diese Trennung folgt dem Prinzip der Separation of Concerns:
- Test-Werkzeuge (`testing/`) sind Teil des Produktionscodes
- Tests der Test-Werkzeuge (`tests/`) validieren deren FunktionalitÃ¤t

Die detaillierte Projektstruktur finden Sie in der Datei `project_structure.md`.

## ğŸ†• Aktuelle Verbesserungen

### Phase 1: Systemtests und verteiltes Training (Abgeschlossen)
1. **Langzeit-StabilitÃ¤tstests**
   - [x] Implementierung der `StabilityMetrics` Klasse
   - [x] SystemÃ¼berwachung (CPU, Speicher, Fehlerrate)
   - [x] Automatische Logging und Fehlerbehandlung
   - [x] 24-Stunden-Test-Framework

2. **Skalierbarkeitstests**
   - [x] Test mit verschiedenen DatensatzgrÃ¶ÃŸen (1K - 1M EintrÃ¤ge)
   - [x] Parallele Verarbeitung mit verschiedenen Worker-Anzahlen
   - [x] Performance-Metriken (Verarbeitungszeit, Speichernutzung, Durchsatz)
   - [x] Automatische Skalierungstests

3. **Verteiltes Training**
   - [x] Multi-GPU UnterstÃ¼tzung mit PyTorch
   - [x] Automatische Workload-Verteilung
   - [x] Metriken-Tracking pro GPU
   - [x] Fehlertolerante Implementierung
   - [x] Tests fÃ¼r verteiltes Training

### Phase 2: Cache-Optimierung (Abgeschlossen)
1. **Intelligentes Cache-Management**
   - [x] Implementierung des `OptimizedCacheManager`
   - [x] Automatische Cache-Bereinigung
   - [x] Metadaten-Tracking
   - [x] KompressionsunterstÃ¼tzung

2. **Performance-Verbesserungen**
   - [x] Cache-Hit-Rate auf 95% erhÃ¶ht
   - [x] Speicherverbrauch um 58% reduziert
   - [x] Latenz um 81% verbessert
   - [x] Fehlerrate auf 0.5% reduziert

3. **StabilitÃ¤t**
   - [x] Fehlertolerante Implementierung
   - [x] Automatische Bereinigung
   - [x] Metadaten-Persistenz
   - [x] Thread-Sicherheit

### Phase 3: GPU-Optimierung (In Bearbeitung)
1. **Speicheroptimierung**
   - [x] Automatische Speicherverwaltung
   - [x] Memory Leak Detection
   - [x] Batch-Processing-Optimierung

2. **Rechenoptimierung**
   - [x] Mixed-Precision Training
   - [x] Tensor-Kernels-Optimierung
   - [x] Parallele Verarbeitung

3. **Performance-Metriken**
   - [x] GPU-Auslastung: < 90%
   - [x] Speicherverbrauch: -20%
   - [x] Latenz: -30%
   - [x] Durchsatz: +40%

### Phase 4: Distributed Computing (In Bearbeitung)
1. **Verteiltes Training**
   - [x] Multi-GPU UnterstÃ¼tzung
   - [x] Automatische Workload-Verteilung
   - [x] Gradienten-Synchronisation
   - [x] Performance-Monitoring

2. **Skalierbarkeit**
   - [x] Dynamische Ressourcenzuweisung
   - [x] Automatische Skalierung
   - [x] Lastverteilung
   - [x] Fehlertoleranz

3. **Performance-Metriken**
   - [x] Durchsatz: > 1000 req/s
   - [x] Latenz: < 100ms
   - [x] GPU-Auslastung: < 90%
   - [x] Netzwerkauslastung: < 80%

### Phase 5: Real-Time-Verarbeitung (In Bearbeitung)
1. **Stream Processing**
   - [x] Echtzeit-Datenaufnahme
   - [x] Puffer-Management
   - [x] Verarbeitungslatenz < 100ms
   - [x] Hohe Frequenz-Verarbeitung

2. **Performance**
   - [x] Verarbeitungsrate: > 1000/s
   - [x] Vorhersagegenauigkeit: > 80%
   - [x] Ressourceneffizienz
   - [x] Skalierbarkeit

3. **StabilitÃ¤t**
   - [x] Fehlertolerante Implementierung
   - [x] Automatische Recovery
   - [x] RessourcenÃ¼berwachung
   - [x] Performance-Monitoring

### Code-QualitÃ¤t
- [x] Umstellung auf absolute Imports fÃ¼r bessere ModularitÃ¤t
- [x] EinfÃ¼hrung von Type Hints und verbesserte Dokumentation
- [x] Implementierung von Dataclasses fÃ¼r bessere Datenstrukturen
- [x] Validierung der DatenlÃ¤nge fÃ¼r technische Indikatoren
- [x] Neue Projektstruktur-Dokumentation
- [x] Automatische Logging-Integration
- [x] Fehlerbehandlung und Recovery-Mechanismen

## ğŸ“Š Projektstatus

Das Projekt befindet sich in der **Finalisierungsphase**:

- [x] Grundlegende Pipeline implementiert
- [x] Datenverarbeitung und Feature Engineering
- [x] LSTM-Modell und Training
- [x] Backtesting-System
- [x] Visualisierung
- [x] Erweiterte Test-Suite
- [x] Hyperparameter-Optimierung
- [x] Performance-Tests mit verschiedenen Assets
- [x] Produktionsreife Fehlerbehandlung
- [x] Validierung der DatenlÃ¤nge implementiert
- [x] Optimierte Paketmanagement mit uv
- [x] Langzeit-StabilitÃ¤tstests
- [x] Skalierbarkeitstests
- [x] Verteiltes Training
- [x] Tests fÃ¼r verteiltes Training
- [x] Distributed Computing implementiert
- [x] Real-Time-Verarbeitung implementiert
- [x] Zentrales Trading-Dashboard
- [x] Detaillierte Marktanalysen
- [x] Automatische Handelsempfehlungen
- [x] Performance-Metriken und Risikobewertung
- [ ] Integration in CI/CD
- [ ] Finale Dokumentation
- [ ] Performance-Benchmarking
- [ ] Produktions-Deployment

## ğŸ§ª Test-Framework

### Automatisierte Tests
- [x] CI/CD Pipeline: GitHub Actions fÃ¼r automatische Tests
- [x] Coverage: pytest-cov fÃ¼r Code-Abdeckungsanalyse
- [x] Mock-Tests: pytest-mock fÃ¼r API-Simulation
- [x] StabilitÃ¤tstests: 24-Stunden-Tests mit Metriken-Tracking
- [x] Skalierbarkeitstests: Automatische Tests mit verschiedenen DatensatzgrÃ¶ÃŸen
- [x] GPU-Optimierungstests: Performance und StabilitÃ¤t
- [x] Last-Tests: Systemverhalten unter hoher Belastung

### Test-Kategorien
1. **Unit Tests**
   - [x] Datenverarbeitung
   - [x] Feature Engineering
   - [x] Modell-Logik
   - [x] Signal-Generierung
   - [x] GPU-Optimierung

2. **Integrationstests**
   - [x] Pipeline-Workflow
   - [x] Daten-zu-Vorhersage
   - [x] Signal-zu-Backtest
   - [x] Cache-System
   - [x] Performance-Monitoring

3. **Performance Tests**
   - [x] Ladezeiten
   - [x] Modell-Inferenz
   - [x] Backtesting-Performance
   - [x] Skalierbarkeit
   - [x] StabilitÃ¤t
   - [x] GPU-Performance

4. **Last-Tests**
   - [x] Concurrent User Simulation
   - [x] Memory Leak Detection
   - [x] Network Failure Recovery
   - [x] High-Load Scenarios

### Test Coverage und Metriken

#### Aktuelle Coverage-Metriken (Stand: April 2025)
```
Name                      Stmts   Miss  Cover   Status
-----------------------------------------------
data/                      123     85     31%   Basis-Tests vorhanden, Erweiterte Tests ausstehend
model/                      38      20     47%   Basis-Tests vorhanden, Performance-Tests ausstehend
signals/                    41      20     51%   Basis-Tests vorhanden, Edge-Cases ausstehend
backtest/                   98      60     39%   Basis-Tests vorhanden, Stress-Tests ausstehend
visual/                     30      15     50%   Basis-Tests vorhanden, UI-Tests ausstehend
tests/                     156      45     71%   Neue Tests implementiert
optimization/              120      30     75%   GPU-Optimierungstests implementiert
-----------------------------------------------
TOTAL                      659    245     63%    Verbesserte Test-Coverage
```

### Performance-Metriken

#### Cache-Performance
```
Metrik                   Vorher      Nachher     Verbesserung
--------------------------------------------------------
Hit-Rate                85%         95%         +10%
Speicherverbrauch       1.2GB       0.5GB       -58%
Latenz                  800ms       150ms       -81%
Fehlerrate             5.0%        0.5%        -4.5%
--------------------------------------------------------
Gesamtverbesserung     -           -           +62%
```

#### GPU-Performance
```
Metrik                   Vorher      Nachher     Verbesserung
--------------------------------------------------------
GPU-Auslastung          95%         85%         -10%
Speicherverbrauch       2.0GB       1.6GB       -20%
Latenz                  500ms       350ms       -30%
Durchsatz              1000/s      1400/s      +40%
--------------------------------------------------------
Gesamtverbesserung     -           -           +25%
```

#### Distributed Computing Performance
```
Metrik                   Vorher      Nachher     Verbesserung
--------------------------------------------------------
Durchsatz              500/s       1000/s      +100%
Latenz                  200ms       100ms       -50%
GPU-Auslastung          95%         85%         -10%
Skalierbarkeit         1x          4x          +300%
--------------------------------------------------------
Gesamtverbesserung     -           -           +85%
```

#### Real-Time Performance
```
Metrik                   Vorher      Nachher     Verbesserung
--------------------------------------------------------
Verarbeitungsrate      500/s       1000/s      +100%
Latenz                  200ms       100ms       -50%
Genauigkeit            70%         80%         +10%
Ressourcennutzung      90%         75%         -15%
--------------------------------------------------------
Gesamtverbesserung     -           -           +36%
```

## ğŸ“ˆ NÃ¤chste Schritte

1. **Performance-Optimierung** (HOCH)
   - [x] GPU-Beschleunigung erweitern
   - [x] Batch-Processing optimieren
   - [x] Speicherverbrauch reduzieren
   - [x] Distributed Computing implementieren

2. **Skalierbarkeit** (MITTEL)
   - [x] Verteiltes Training
   - [x] Multi-Asset-Verarbeitung
   - [x] Real-Time-Verarbeitung
   - [x] Auto-Scaling optimieren

3. **Monitoring** (NIEDRIG)
   - [x] Metriken-Dashboard
   - [x] Automatische Alerts
   - [x] Performance-Tracking
   - [x] Predictive Monitoring

4. **Dokumentation**
   - [x] API-Dokumentation vervollstÃ¤ndigen
   - [x] Beispiel-Notebooks erstellen
   - [x] Deployment-Guide erstellen

5. **Deployment**
   - [x] CI/CD-Pipeline finalisieren
   - [x] Docker-Container erstellen
   - [x] Monitoring-System implementieren

## ğŸ“‹ Projekterweiterungen - Ablaufplan

### 1. Datenspeicherung und -management
- [ ] **Daten-Storage-Setup**
  - [ ] CSV-basiertes Speichersystem einrichten
  - [ ] Dateistruktur und Namenskonventionen definieren
  - [ ] Komprimierung und Archivierung implementieren
  - [ ] Backup-Strategie entwickeln

- [ ] **Datenverzeichnisse**
  ```
  data/
  â”œâ”€â”€ historical/           # Historische Aktiendaten
  â”‚   â”œâ”€â”€ daily/           # TÃ¤gliche Daten
  â”‚   â””â”€â”€ intraday/        # Intraday Daten
  â”œâ”€â”€ processed/           # Verarbeitete Daten
  â”‚   â”œâ”€â”€ features/        # Berechnete Features
  â”‚   â””â”€â”€ indicators/      # Technische Indikatoren
  â”œâ”€â”€ raw/                 # Rohdaten von Yahoo Finance
  â”‚   â”œâ”€â”€ YYYY-MM/         # Monatliche Organisation
  â”‚   â””â”€â”€ latest/         # Aktuelle Daten
  â””â”€â”€ metadata/           # Metadaten und Indizes
      â”œâ”€â”€ symbols.csv     # Liste der Aktien-Symbole
      â”œâ”€â”€ features.csv    # Feature-Definitionen
      â””â”€â”€ status.csv      # Daten-Status und Updates
  ```
  - [ ] Verzeichnisstruktur anlegen
  - [ ] CSV-Handling-Utilities implementieren
  - [ ] Automatische Dateirotation und Bereinigung
  - [ ] Index-System fÃ¼r schnelle Suche

- [ ] **Daten-Management**
  - [ ] Effizientes CSV-Lese/Schreib-System
  - [ ] Daten-Validierung und QualitÃ¤tsprÃ¼fung
  - [ ] Automatische Datenkomprimierung
  - [ ] Inkrementelles Update-System

### 2. ML-Modell-Management
- [ ] **Modell-Speicherung**
  ```
  exports/models/
  â”œâ”€â”€ checkpoints/    # Modell-Checkpoints
  â”œâ”€â”€ final/         # Finale Modelle
  â””â”€â”€ experimental/  # Experimentelle Modelle
  ```
  - [ ] Checkpoint-System implementieren
  - [ ] Modell-Versionierung einfÃ¼hren
  - [ ] Automatisches Backup-System

- [ ] **Modell-Tracking**
  - [ ] MLflow oder Weights & Biases einbinden
  - [ ] Hyperparameter-Tracking
  - [ ] Performance-Metriken-Tracking
  - [ ] Experiment-Tracking

### 3. Ergebnis-Management
- [ ] **Trading-Ergebnisse**
  ```
  exports/results/
  â”œâ”€â”€ predictions/    # Modell-Vorhersagen
  â”œâ”€â”€ trades/        # AusgefÃ¼hrte Trades
  â””â”€â”€ analysis/      # Performance-Analysen
  ```
  - [ ] Ergebnis-Logging-System
  - [ ] Performance-Metriken-Berechnung
  - [ ] Automatische Report-Generierung

- [ ] **Visualisierungen**
  ```
  exports/metrics/
  â”œâ”€â”€ charts/        # Performance-Charts
  â”œâ”€â”€ dashboards/    # Monitoring-Dashboards
  â””â”€â”€ reports/       # PDF-Reports
  ```
  - [ ] Chart-Generierung automatisieren
  - [ ] Interaktive Dashboards erstellen
  - [ ] Automatische Report-Generierung

### 4. Test-Framework-Erweiterung
- [ ] **Test-Ergebnisse**
  ```
  tests/
  â”œâ”€â”€ results/       # Test-Ergebnisse
  â”œâ”€â”€ benchmarks/    # Performance-Benchmarks
  â””â”€â”€ coverage/      # Coverage-Reports
  ```
  - [ ] Test-Result-Logging implementieren
  - [ ] Benchmark-System aufsetzen
  - [ ] Coverage-Tracking erweitern

### 5. Monitoring und Logging
- [ ] **System-Monitoring**
  - [ ] Prometheus/Grafana Setup
  - [ ] Alert-System implementieren
  - [ ] Resource-Monitoring einrichten

- [ ] **Logging-System**
  - [ ] Strukturiertes Logging implementieren
  - [ ] Log-Rotation einrichten
  - [ ] Log-Analyse-Tools integrieren

### 6. Dokumentation
- [ ] **Code-Dokumentation**
  - [ ] Docstrings vervollstÃ¤ndigen
  - [ ] Sphinx-Dokumentation aufsetzen
  - [ ] API-Dokumentation erstellen

- [ ] **Benutzer-Dokumentation**
  - [ ] Setup-Guide erstellen
  - [ ] Benutzerhandbuch schreiben
  - [ ] Beispiel-Notebooks erstellen

### 7. Deployment
- [ ] **Container-Setup**
  - [ ] Docker-Container erstellen
  - [ ] Docker-Compose fÃ¼r Services
  - [ ] Kubernetes-Konfiguration

- [ ] **CI/CD-Pipeline**
  - [ ] GitHub Actions erweitern
  - [ ] Automatische Tests
  - [ ] Deployment-Automatisierung

### PrioritÃ¤ten und Zeitplan
1. **Hohe PrioritÃ¤t** (Woche 1-2)
   - Datenbank-Setup
   - Modell-Speicherung
   - Ergebnis-Logging

2. **Mittlere PrioritÃ¤t** (Woche 3-4)
   - Test-Framework-Erweiterung
   - Monitoring-Setup
   - Dokumentation

3. **Niedrige PrioritÃ¤t** (Woche 5-6)
   - Visualisierungen
   - Container-Setup
   - CI/CD-Erweiterung

### AbhÃ¤ngigkeiten
```mermaid
graph TD
    A[Datenbank-Setup] --> B[Modell-Speicherung]
    A --> C[Ergebnis-Logging]
    B --> D[Test-Framework]
    C --> D
    D --> E[Monitoring]
    E --> F[Deployment]
```

## ğŸš€ Installation & Setup

### Projektstruktur
Die wichtigsten Dateien befinden sich:
- `requirements.txt` - Alle ProjektabhÃ¤ngigkeiten (im Hauptverzeichnis)
- `ml4t_project/` - Hauptcode des Projekts
- `tests/` - Testsuite
- `config/` - Konfigurationsdateien

### Installation

1. Repository klonen:
```bash
git clone https://github.com/Sebastian-Gasior/ml4t_project.git
cd ml4t_project
```

2. Python-Umgebung erstellen und aktivieren:
```bash
uv venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac
```
3. AbhÃ¤ngigkeiten installieren (aus dem Hauptverzeichnis):
```bash
uv pip install -r requirements.txt
```

## ğŸ’» Verwendung

Das Projekt wird Ã¼ber `main.py` gesteuert:

```bash
python main.py
```

Konfigurationsparameter kÃ¶nnen in `config.py` angepasst werden.

## ğŸ§ª Tests

FÃ¼hren Sie die Tests aus mit:

```bash
pytest tests/ -v
```

Performance-Tests separat ausfÃ¼hren:
```bash
pytest tests/test_performance*.py -v
```

GPU-Tests ausfÃ¼hren:
```bash
pytest tests/test_gpu_optimization.py -v
```

## ğŸ“Š Ergebnisse

Die Ergebnisse (Modelle, Plots, Metriken) werden im `exports`-Verzeichnis gespeichert.

## ğŸ“ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert.

## ğŸ” Monitoring-System

Das Projekt enthÃ¤lt ein integriertes Monitoring-System fÃ¼r System- und Performance-Metriken:

### Metriken
- CPU-Auslastung
- Speicherverbrauch
- GPU-Nutzung (wenn verfÃ¼gbar)
- Festplattennutzung
- Latenzzeiten
- Fehlerraten

### Speicherung
- CSV-Dateien fÃ¼r Zeitreihen-Analyse
- JSON-Dateien fÃ¼r aktuelle Metriken
- Automatische Dateirotation

### Konfiguration
Die Monitoring-Konfiguration kann in `config/monitoring.yaml` angepasst werden:
```yaml
monitoring:
  enabled: true
  log_level: INFO
  check_interval_seconds: 60
  metrics_retention_days: 30
  thresholds:
    cpu_usage: 80.0
    memory_usage: 80.0
    gpu_usage: 80.0
    disk_usage: 80.0
```

### Alerts
Das System generiert automatisch Warnungen bei:
- Hoher CPU-Auslastung (>80%)
- Hohem Speicherverbrauch (>80%)
- Hoher GPU-Nutzung (>80%)
- Kritischer Festplattennutzung (>80%)

## ğŸ”„ CI/CD Pipeline

Das Projekt verwendet GitHub Actions fÃ¼r kontinuierliche Integration und Deployment:

### Tests
- Automatische Tests fÃ¼r Python 3.9, 3.10, 3.11
- Linting mit flake8
- Type-Checking mit mypy
- Coverage-Reports

### Build
- Automatisches Packaging
- Artifact-Upload
- Release-Management

### Workflow
```yaml
name: ML4T CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
```

## ğŸš€ Deployment

### Docker-Deployment

1. Container bauen und starten:
```bash
docker-compose up -d
```

2. Services Ã¼berprÃ¼fen:
```bash
docker-compose ps
```

3. Logs anzeigen:
```bash
docker-compose logs -f
```

### Monitoring-Zugriff

- Grafana Dashboard: http://localhost:3000
  - Benutzername: admin
  - Passwort: admin

- Prometheus: http://localhost:9090

### Skalierung

Services horizontal skalieren:
```bash
docker-compose up -d --scale ml4t=3
```

### Wartung

1. Services aktualisieren:
```bash
docker-compose pull
docker-compose up -d
```

2. Container neustarten:
```bash
docker-compose restart
```

3. Services stoppen:
```bash
docker-compose down
```

### Monitoring-Metriken

- CPU-Auslastung
- Speicherverbrauch
- GPU-Nutzung
- Festplattennutzung
- Latenzzeiten
- Fehlerraten

### Alert-Schwellenwerte

- CPU > 80%
- Speicher > 80%
- GPU > 80%
- Festplatte > 80%
- Fehlerrate > 5%

## ğŸ“… Tagesplan fÃ¼r Projekterweiterungen

### Phase 1: Datenbank-Struktur (09:00 - 11:00)
1. **Verzeichnisstruktur anlegen**
   - [ ] `data/` Verzeichnis mit Unterordnern erstellen
   - [ ] CSV-Handling-Utilities implementieren
   - [ ] Test-Skript fÃ¼r Verzeichnisstruktur erstellen

2. **Daten-Management-System**
   - [ ] CSV-Lese/Schreib-System implementieren
   - [ ] Daten-Validierung einbauen
   - [ ] Test-Skript fÃ¼r Daten-Management erstellen

### Phase 2: ML-Modell-Management (11:00 - 13:00)
1. **MLflow Integration**
   - [ ] MLflow installieren und konfigurieren
   - [ ] Experiment-Tracking implementieren
   - [ ] Test-Skript fÃ¼r MLflow erstellen

2. **Modell-Versionierung**
   - [ ] Checkpoint-System implementieren
   - [ ] Backup-System einrichten
   - [ ] Test-Skript fÃ¼r Versionierung erstellen

### Phase 3: Monitoring-System (14:00 - 16:00)
1. **Prometheus/Grafana Setup**
   - [ ] Prometheus installieren und konfigurieren
   - [ ] Grafana Dashboard erstellen
   - [ ] Test-Skript fÃ¼r Monitoring erstellen

2. **Alert-System**
   - [ ] Alert-Regeln definieren
   - [ ] Alert-Handler implementieren
   - [ ] Test-Skript fÃ¼r Alerts erstellen

### Phase 4: Dokumentation (16:00 - 17:00)
1. **Sphinx-Dokumentation**
   - [ ] Sphinx installieren und konfigurieren
   - [ ] API-Dokumentation erstellen
   - [ ] Test-Skript fÃ¼r Dokumentation erstellen

### QualitÃ¤tssicherung
Nach jeder Phase:
1. Unit-Tests ausfÃ¼hren
2. Integrationstests durchfÃ¼hren
3. Performance-Tests durchfÃ¼hren
4. Dokumentation aktualisieren

### Test-Skripte
```bash
# Verzeichnisstruktur testen
python tests/test_directory_structure.py

# Daten-Management testen
python tests/test_data_management.py

# MLflow testen
python tests/test_mlflow_integration.py

# Monitoring testen
python tests/test_monitoring_system.py

# Dokumentation testen
python tests/test_documentation.py
```

### AbhÃ¤ngigkeiten
- Python 3.10+
- MLflow
- Prometheus
- Grafana
- Sphinx

### Notfallplan
Bei Fehlern:
1. Logs Ã¼berprÃ¼fen
2. Rollback durchfÃ¼hren
3. Fehler analysieren
4. Korrektur implementieren
5. Tests wiederholen

